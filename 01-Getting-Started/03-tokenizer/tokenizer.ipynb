{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Envs\\py_cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = '弱小的我也有大梦想!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer保存到本地\n",
    "# tokenizer.save_pretrained('./roberta_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../../models/roberta-base-finetuned-dianping-chinese/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../../models/roberta-base-finetuned-dianping-chinese/')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##come': 12097,\n",
       " '##拜': 15933,\n",
       " '##適': 19957,\n",
       " '讧': 6373,\n",
       " '##閥': 20342,\n",
       " '##侵': 13966,\n",
       " '##;': 13333,\n",
       " '##挙': 15963,\n",
       " '##杠': 16396,\n",
       " '飛': 7606,\n",
       " '##悩': 15702,\n",
       " '銃': 7066,\n",
       " '##喔': 14652,\n",
       " '##丸': 13766,\n",
       " '污': 3738,\n",
       " 'street': 9471,\n",
       " '##煉': 17257,\n",
       " '##腿': 18654,\n",
       " '##con': 10281,\n",
       " '闵': 7314,\n",
       " '亚': 762,\n",
       " '8591': 12376,\n",
       " '淩': 3913,\n",
       " '漣': 4032,\n",
       " '牒': 4279,\n",
       " '##lin': 9029,\n",
       " '##尋': 15261,\n",
       " '##犧': 17361,\n",
       " '##ina': 9314,\n",
       " '##倾': 14024,\n",
       " 'w': 165,\n",
       " '##hy': 9943,\n",
       " '##煙': 17263,\n",
       " '設': 6257,\n",
       " '##哺': 14586,\n",
       " '326': 12044,\n",
       " '##寄': 15221,\n",
       " '蓄': 5898,\n",
       " '渴': 3951,\n",
       " '鶩': 7872,\n",
       " 'low': 10611,\n",
       " '枣': 3365,\n",
       " '赓': 6607,\n",
       " '##冈': 14139,\n",
       " '##压': 14384,\n",
       " '282': 11431,\n",
       " '##嗔': 14682,\n",
       " 'mail': 8313,\n",
       " '莹': 5816,\n",
       " '##奔': 15001,\n",
       " '##暱': 16330,\n",
       " '茭': 5757,\n",
       " 'traction': 12600,\n",
       " '╱': 447,\n",
       " '[unused44]': 44,\n",
       " '嘣': 1663,\n",
       " 'dear': 11694,\n",
       " '##rame': 13185,\n",
       " '##寞': 15231,\n",
       " '##娴': 15090,\n",
       " '枷': 3374,\n",
       " 'img': 11412,\n",
       " '##ky': 10218,\n",
       " '１５': 11213,\n",
       " '##をお': 11236,\n",
       " '##hip': 11489,\n",
       " '##ince': 13199,\n",
       " '##凄': 14170,\n",
       " 'ᆸ': 325,\n",
       " '##悱': 15706,\n",
       " '338': 11929,\n",
       " '##飓': 20658,\n",
       " 'more': 8384,\n",
       " '##餃': 20675,\n",
       " '##鈔': 20102,\n",
       " '杞': 3337,\n",
       " '友': 1351,\n",
       " '悄': 2632,\n",
       " '犄': 4299,\n",
       " '##lux': 12700,\n",
       " '##淞': 16965,\n",
       " '伐': 827,\n",
       " '卓': 1294,\n",
       " '﹐': 8000,\n",
       " 'ios': 8276,\n",
       " '厌': 1328,\n",
       " '乐': 727,\n",
       " '亟': 766,\n",
       " '帕': 2364,\n",
       " '擲': 3096,\n",
       " 'item': 9279,\n",
       " 'lohas': 9757,\n",
       " '##秣': 17967,\n",
       " '222': 9918,\n",
       " '##穫': 18010,\n",
       " '占': 1304,\n",
       " '##箐': 18104,\n",
       " '姥': 2005,\n",
       " '姉': 1991,\n",
       " '狡': 4322,\n",
       " '算': 5050,\n",
       " '##施': 16234,\n",
       " '谢': 6468,\n",
       " '##消': 16924,\n",
       " '##親': 19274,\n",
       " '##觀': 19280,\n",
       " '##摟': 16095,\n",
       " '囟': 1727,\n",
       " '偿': 985,\n",
       " '挠': 2912,\n",
       " '##动': 14277,\n",
       " '##松': 16408,\n",
       " '氦': 3708,\n",
       " '霽': 7466,\n",
       " '810': 11296,\n",
       " '昕': 3213,\n",
       " '##咏': 14528,\n",
       " '麴': 7933,\n",
       " '##睨': 17779,\n",
       " '##蹼': 19763,\n",
       " '##槁': 16594,\n",
       " '##time': 9785,\n",
       " '##馴': 20740,\n",
       " '##ira': 11630,\n",
       " '##濃': 17140,\n",
       " '##鮪': 20859,\n",
       " '潘': 4050,\n",
       " '##求': 16781,\n",
       " '##卢': 14363,\n",
       " '女': 1957,\n",
       " 'forest': 10889,\n",
       " '##鹊': 20960,\n",
       " '暄': 3258,\n",
       " '羣': 5407,\n",
       " '##域': 14875,\n",
       " '##鼠': 21019,\n",
       " '►': 467,\n",
       " '##h': 8199,\n",
       " '弑': 2467,\n",
       " '瓦': 4482,\n",
       " '##core': 11921,\n",
       " '恪': 2618,\n",
       " '##揩': 16054,\n",
       " '##洄': 16873,\n",
       " '##獗': 17414,\n",
       " '##鸠': 20939,\n",
       " '##聾': 18539,\n",
       " '##ia': 8321,\n",
       " 'miss': 9368,\n",
       " 'ᄁ': 289,\n",
       " '##洪': 16882,\n",
       " '##暖': 16322,\n",
       " '嚏': 1704,\n",
       " '##乱': 13801,\n",
       " '∕': 378,\n",
       " '##缘': 18414,\n",
       " '煦': 4211,\n",
       " '334': 12225,\n",
       " '谓': 6458,\n",
       " '##兢': 14113,\n",
       " '##剋': 14239,\n",
       " '##闔': 20355,\n",
       " '筝': 5034,\n",
       " '碱': 4822,\n",
       " '硼': 4804,\n",
       " '##ア': 9788,\n",
       " '050': 11972,\n",
       " '##積': 18005,\n",
       " 'linkedin': 11369,\n",
       " 'avira': 10473,\n",
       " '920': 11976,\n",
       " '泼': 3812,\n",
       " '##党': 14111,\n",
       " '雏': 7422,\n",
       " '##秭': 17972,\n",
       " '麺': 7936,\n",
       " '##気': 16757,\n",
       " 'part': 9124,\n",
       " '摳': 3042,\n",
       " 'population': 13271,\n",
       " '##讽': 19448,\n",
       " '畀': 4516,\n",
       " 'cto': 12633,\n",
       " '##范': 18802,\n",
       " '##裘': 19226,\n",
       " '##費': 19584,\n",
       " '仏': 795,\n",
       " '畫': 4529,\n",
       " '铿': 7217,\n",
       " '爵': 4265,\n",
       " '券': 1171,\n",
       " '宫': 2151,\n",
       " '矯': 4766,\n",
       " '詮': 6280,\n",
       " '週': 6867,\n",
       " 'cheese': 11387,\n",
       " 'nbapop': 12188,\n",
       " '尺': 2223,\n",
       " '##!': 13317,\n",
       " '##匍': 14319,\n",
       " '##晤': 16302,\n",
       " '##眷': 17760,\n",
       " '##荘': 18834,\n",
       " '##央': 14982,\n",
       " '腹': 5592,\n",
       " '吳': 1425,\n",
       " '阶': 7348,\n",
       " 'weibo': 10565,\n",
       " '364': 12673,\n",
       " '##午': 14343,\n",
       " '##蠱': 19170,\n",
       " '##鸾': 20952,\n",
       " '##鄺': 20033,\n",
       " '##蠹': 19172,\n",
       " '饕': 7642,\n",
       " '##疔': 17600,\n",
       " '##邯': 19992,\n",
       " '##酷': 20056,\n",
       " '模': 3563,\n",
       " '遜': 6893,\n",
       " '##川': 15392,\n",
       " '日': 3189,\n",
       " 'a7': 11226,\n",
       " '##丰': 13762,\n",
       " '##年': 15456,\n",
       " '隙': 7395,\n",
       " '獻': 4368,\n",
       " '賑': 6539,\n",
       " '##熾': 17289,\n",
       " '##test': 11574,\n",
       " '##竇': 18044,\n",
       " '##ail': 10923,\n",
       " '垛': 1801,\n",
       " '诗': 6408,\n",
       " '##labels': 10636,\n",
       " '##勵': 14309,\n",
       " '##ics': 9034,\n",
       " '##鸞': 20937,\n",
       " '叔': 1356,\n",
       " '甲': 4508,\n",
       " '##ŋ': 13366,\n",
       " '繫': 5258,\n",
       " '##row': 10570,\n",
       " '尘': 2212,\n",
       " 'ar': 8673,\n",
       " '##おります': 9797,\n",
       " '##応': 15622,\n",
       " 'old': 10404,\n",
       " 'dj': 9135,\n",
       " '##届': 15294,\n",
       " 'bot': 11825,\n",
       " '2050': 11931,\n",
       " 'doc': 9656,\n",
       " '盼': 4687,\n",
       " '##穹': 18014,\n",
       " 'c3': 11829,\n",
       " '##麾': 20997,\n",
       " '劾': 1231,\n",
       " 'turbo': 12325,\n",
       " 'ofo': 9574,\n",
       " 'mu': 11677,\n",
       " '揃': 2984,\n",
       " '##host': 12227,\n",
       " '##末': 16371,\n",
       " '##均': 14829,\n",
       " '##笨': 18074,\n",
       " '##嘛': 14715,\n",
       " '關': 7302,\n",
       " '##я': 12992,\n",
       " '##ⁿ': 13513,\n",
       " '##sco': 11364,\n",
       " '璃': 4461,\n",
       " '﹑': 8001,\n",
       " '寰': 2186,\n",
       " '葡': 5868,\n",
       " 'twitter': 8266,\n",
       " '##样': 16473,\n",
       " '##巽': 15409,\n",
       " 'cba': 10912,\n",
       " '##坪': 14847,\n",
       " '##贪': 19633,\n",
       " 'ᅥ': 307,\n",
       " 'ᵃ': 330,\n",
       " '##律': 15583,\n",
       " '丰': 705,\n",
       " '妳': 1986,\n",
       " '贮': 6580,\n",
       " '篱': 5075,\n",
       " 'say': 10114,\n",
       " '##靡': 20537,\n",
       " '饍': 7639,\n",
       " '##处': 14962,\n",
       " 'si': 10883,\n",
       " '##殷': 16725,\n",
       " '216': 9972,\n",
       " 'from': 8670,\n",
       " '##倫': 14018,\n",
       " '##帶': 15437,\n",
       " 'make': 9994,\n",
       " '坵': 1793,\n",
       " '##刷': 14227,\n",
       " '阂': 7324,\n",
       " '菏': 5827,\n",
       " '##talk': 12534,\n",
       " '夾': 1933,\n",
       " '##stin': 12921,\n",
       " '##︶': 21051,\n",
       " '##茨': 18811,\n",
       " '怙': 2588,\n",
       " '纂': 5264,\n",
       " 'da': 10005,\n",
       " '121': 9247,\n",
       " '慚': 2712,\n",
       " '地': 1765,\n",
       " '##锭': 20297,\n",
       " '軽': 6731,\n",
       " '鼾': 7966,\n",
       " 'ｸ': 8089,\n",
       " '##佔': 13918,\n",
       " '##壕': 14944,\n",
       " '##拴': 15949,\n",
       " '葛': 5867,\n",
       " '雷': 7440,\n",
       " '##穴': 18011,\n",
       " '##较': 19829,\n",
       " '##买': 13800,\n",
       " 'marc': 11511,\n",
       " '瞭': 4747,\n",
       " '588': 12426,\n",
       " '##奴': 15015,\n",
       " '##鰻': 20873,\n",
       " 'face': 10656,\n",
       " '##接': 16027,\n",
       " '驗': 7710,\n",
       " '##繼': 18319,\n",
       " '##ち': 9967,\n",
       " 'eia': 11981,\n",
       " '郑': 6948,\n",
       " 'bella': 13192,\n",
       " '﹣': 8012,\n",
       " 'official': 12863,\n",
       " '##»': 13359,\n",
       " '##锄': 20278,\n",
       " '苛': 5729,\n",
       " '錮': 7096,\n",
       " '赵': 6627,\n",
       " '遣': 6897,\n",
       " '钾': 7185,\n",
       " '巴': 2349,\n",
       " '照': 4212,\n",
       " '##?': 13337,\n",
       " '##聋': 18522,\n",
       " '干': 2397,\n",
       " 'photo': 9020,\n",
       " '##販': 19573,\n",
       " '##饅': 20692,\n",
       " '籃': 5091,\n",
       " '##割': 14257,\n",
       " '鲈': 7827,\n",
       " '门': 7305,\n",
       " '读': 6438,\n",
       " 'amp': 9985,\n",
       " '問': 1558,\n",
       " 'uv': 9473,\n",
       " '##tory': 12608,\n",
       " '寂': 2163,\n",
       " '鱉': 7819,\n",
       " '##閔': 20337,\n",
       " '##嚏': 14761,\n",
       " '##后': 14457,\n",
       " '淆': 3898,\n",
       " '##剿': 14261,\n",
       " '協': 1295,\n",
       " '瀞': 4111,\n",
       " '免': 1048,\n",
       " '##ob': 11047,\n",
       " '儉': 1025,\n",
       " 'paypal': 8657,\n",
       " '隽': 7409,\n",
       " '##噻': 14756,\n",
       " '##竿': 18061,\n",
       " '##痛': 17635,\n",
       " '##014': 11365,\n",
       " '##铡': 20258,\n",
       " '岱': 2276,\n",
       " '辫': 6797,\n",
       " 'mod': 9650,\n",
       " 'channel': 11261,\n",
       " '##俾': 13997,\n",
       " '##踢': 19734,\n",
       " '云': 756,\n",
       " 'etf': 8946,\n",
       " 'cpa': 11963,\n",
       " '##甜': 17551,\n",
       " '枚': 3361,\n",
       " '##閱': 20345,\n",
       " '醺': 7020,\n",
       " '✨': 501,\n",
       " '##澤': 17132,\n",
       " 'ᄂ': 290,\n",
       " '##刈': 14205,\n",
       " '罗': 5384,\n",
       " '##とう': 12471,\n",
       " '##洵': 16886,\n",
       " '胁': 5516,\n",
       " '嫵': 2077,\n",
       " '鰭': 7814,\n",
       " '##奋': 14996,\n",
       " '710': 12415,\n",
       " '##冶': 14163,\n",
       " '泷': 3809,\n",
       " '矽': 4769,\n",
       " '##镀': 20306,\n",
       " '®': 179,\n",
       " '##脍': 18608,\n",
       " '寮': 2185,\n",
       " '啱': 1575,\n",
       " '铨': 7205,\n",
       " '壇': 1883,\n",
       " '坨': 1789,\n",
       " 'wma': 9900,\n",
       " '##┣': 13585,\n",
       " '##呻': 14517,\n",
       " '嬢': 2084,\n",
       " '榱': 3533,\n",
       " '##拡': 15935,\n",
       " 'honey': 13299,\n",
       " '##out': 9408,\n",
       " '##呲': 14512,\n",
       " '##瀅': 17157,\n",
       " '邇': 6918,\n",
       " 'huawei': 12247,\n",
       " 'fa': 12289,\n",
       " 'singapore': 12916,\n",
       " '##別': 14219,\n",
       " '##try': 11589,\n",
       " '拗': 2871,\n",
       " 'tmt': 11920,\n",
       " '隍': 7388,\n",
       " '##鎚': 20174,\n",
       " '轿': 6771,\n",
       " '夸': 1930,\n",
       " '##骨': 20812,\n",
       " '蜂': 6044,\n",
       " '煥': 4210,\n",
       " '吡': 1414,\n",
       " '囑': 1720,\n",
       " '齣': 7974,\n",
       " '##www': 9718,\n",
       " '餾': 7634,\n",
       " 'c5': 12648,\n",
       " '##朧': 16368,\n",
       " '##360': 10408,\n",
       " '##稻': 17997,\n",
       " '殺': 3669,\n",
       " 'オーフン5': 11810,\n",
       " '躪': 6715,\n",
       " '##榨': 16586,\n",
       " 'ocean': 12546,\n",
       " '♫': 494,\n",
       " '慾': 2725,\n",
       " 'より': 10230,\n",
       " '##墀': 14918,\n",
       " '##龙': 21044,\n",
       " '##ey': 8603,\n",
       " '##07': 9131,\n",
       " '##钒': 20213,\n",
       " '##谬': 19534,\n",
       " 'port': 11311,\n",
       " '药': 5790,\n",
       " '釉': 7024,\n",
       " 'ゥ': 592,\n",
       " '##炁': 17195,\n",
       " '亜': 764,\n",
       " '##via': 9490,\n",
       " '##达': 19866,\n",
       " '##棚': 16533,\n",
       " '抓': 2831,\n",
       " '狱': 4328,\n",
       " '##35': 8852,\n",
       " '3600': 10517,\n",
       " 'ᅴ': 319,\n",
       " '##哌': 14566,\n",
       " '御': 2539,\n",
       " 'step3': 9434,\n",
       " 'states': 11603,\n",
       " '##∕': 13530,\n",
       " '##潋': 17103,\n",
       " '##ria': 10159,\n",
       " '##祟': 17926,\n",
       " '静': 7474,\n",
       " '##貼': 19585,\n",
       " '婕': 2041,\n",
       " '镐': 7256,\n",
       " '敷': 3148,\n",
       " '##ae': 10361,\n",
       " '##鄢': 20027,\n",
       " 'っている': 11859,\n",
       " 'edward': 12259,\n",
       " '记': 6381,\n",
       " '饌': 7638,\n",
       " '##肚': 18553,\n",
       " '奇': 1936,\n",
       " '##te': 8299,\n",
       " '131': 9403,\n",
       " '淹': 3922,\n",
       " '惴': 2683,\n",
       " '##揆': 16043,\n",
       " '壢': 1891,\n",
       " '缺': 5375,\n",
       " '30000': 11158,\n",
       " '瓣': 4480,\n",
       " '靜': 7477,\n",
       " '擷': 3098,\n",
       " '##瀚': 17165,\n",
       " '##遮': 19959,\n",
       " '頡': 7528,\n",
       " '##012': 12037,\n",
       " 'my': 8422,\n",
       " '庚': 2423,\n",
       " '孃': 2093,\n",
       " '晤': 3245,\n",
       " '搂': 3008,\n",
       " '瘩': 4609,\n",
       " '##癥': 17682,\n",
       " '膺': 5613,\n",
       " '1946': 9120,\n",
       " '纨': 5278,\n",
       " '##ย': 13446,\n",
       " '薇': 5948,\n",
       " '漸': 4041,\n",
       " '觊': 6231,\n",
       " '滯': 4015,\n",
       " '舰': 5664,\n",
       " '##plus': 9682,\n",
       " '##玷': 17445,\n",
       " '凛': 1123,\n",
       " '妤': 1979,\n",
       " '体': 860,\n",
       " '婪': 2046,\n",
       " '##昼': 16283,\n",
       " '##绢': 18379,\n",
       " '##啱': 14632,\n",
       " '##帐': 15419,\n",
       " 'nbc': 12141,\n",
       " '##黃': 20998,\n",
       " 'ig': 8288,\n",
       " '[unused80]': 80,\n",
       " '曉': 3280,\n",
       " 'claire': 9753,\n",
       " '##藏': 19023,\n",
       " '##衄': 19175,\n",
       " '258': 10870,\n",
       " '##產': 17553,\n",
       " '##ne': 8354,\n",
       " '6gb': 12324,\n",
       " '筊': 5024,\n",
       " '##遠': 19952,\n",
       " '##pm': 9280,\n",
       " '绣': 5323,\n",
       " '蝦': 6076,\n",
       " '葩': 5871,\n",
       " '##bot': 10119,\n",
       " '謗': 6339,\n",
       " '##ᆻ': 13487,\n",
       " '瀉': 4102,\n",
       " '##蕃': 18988,\n",
       " '婧': 2045,\n",
       " '邀': 6913,\n",
       " '1947': 9253,\n",
       " '##nter': 11526,\n",
       " 'psp': 11337,\n",
       " '##崔': 15360,\n",
       " '##ﾗ': 21117,\n",
       " '##幀': 15442,\n",
       " '##挎': 15960,\n",
       " '靂': 7468,\n",
       " '##跑': 19708,\n",
       " '##浔': 16907,\n",
       " '本': 3315,\n",
       " '##¹': 13357,\n",
       " '譯': 6358,\n",
       " '##rma': 13156,\n",
       " '繕': 5252,\n",
       " '##皂': 17694,\n",
       " '璁': 4460,\n",
       " '##蚱': 19079,\n",
       " '便': 912,\n",
       " '##星': 16272,\n",
       " '##槌': 16597,\n",
       " '葉': 5864,\n",
       " 'asics': 11849,\n",
       " '##私': 17957,\n",
       " '擠': 3089,\n",
       " '壁': 1880,\n",
       " '蕉': 5933,\n",
       " 'education': 12727,\n",
       " 'kde': 12543,\n",
       " '皈': 4641,\n",
       " '##旭': 16252,\n",
       " '肥': 5503,\n",
       " '荻': 5794,\n",
       " '##豫': 19556,\n",
       " '415': 12114,\n",
       " '庇': 2413,\n",
       " '麾': 7940,\n",
       " '##樵': 16627,\n",
       " '##姆': 15047,\n",
       " '230': 9111,\n",
       " '##sday': 12956,\n",
       " '##以': 13866,\n",
       " '磕': 4833,\n",
       " '##审': 15201,\n",
       " '##榕': 16582,\n",
       " '##俄': 13972,\n",
       " 'ﾝ': 8097,\n",
       " 'qe': 11534,\n",
       " '##谗': 19518,\n",
       " '##斂': 16207,\n",
       " '曄': 3277,\n",
       " 'root': 8859,\n",
       " '##尹': 15279,\n",
       " '枯': 3369,\n",
       " '##廈': 15498,\n",
       " '##筝': 18091,\n",
       " '855': 10998,\n",
       " '205': 9860,\n",
       " '##rc': 10227,\n",
       " '秸': 4918,\n",
       " '##烯': 17236,\n",
       " '衆': 6120,\n",
       " '美': 5401,\n",
       " '笺': 5020,\n",
       " '##涞': 16934,\n",
       " '感': 2697,\n",
       " '档': 3440,\n",
       " '##霄': 20503,\n",
       " '1980': 8499,\n",
       " '##两': 13754,\n",
       " '劊': 1209,\n",
       " 'evolution': 12691,\n",
       " '针': 7151,\n",
       " '404': 10524,\n",
       " '糖': 5131,\n",
       " '##峒': 15339,\n",
       " '##亩': 13831,\n",
       " 'forum': 11730,\n",
       " '##腊': 18629,\n",
       " '##潔': 17106,\n",
       " 'っ': 553,\n",
       " '斋': 3153,\n",
       " '簷': 5085,\n",
       " '##審': 15239,\n",
       " '##羽': 18474,\n",
       " '##驳': 20779,\n",
       " '姹': 2011,\n",
       " '羯': 5413,\n",
       " '##賂': 19590,\n",
       " '組': 5175,\n",
       " '##011': 12424,\n",
       " '##wang': 10365,\n",
       " '100m': 13037,\n",
       " '籐': 5094,\n",
       " '〝': 531,\n",
       " '闹': 7317,\n",
       " '猩': 4342,\n",
       " '##dar': 12354,\n",
       " '##想': 15739,\n",
       " '##皿': 17711,\n",
       " '##ᅳ': 13482,\n",
       " '##綁': 18249,\n",
       " 'oh': 9941,\n",
       " '##ya': 8741,\n",
       " '##卟': 14360,\n",
       " '峒': 2282,\n",
       " '95': 8287,\n",
       " 'ての': 9697,\n",
       " '贼': 6592,\n",
       " '嘞': 1660,\n",
       " 'tower': 11102,\n",
       " '##頓': 20581,\n",
       " '烨': 4174,\n",
       " '##ola': 12653,\n",
       " '##ｌ': 12035,\n",
       " '湖': 3959,\n",
       " 'seven': 12684,\n",
       " '##局': 15286,\n",
       " '##慑': 15766,\n",
       " '##op': 9133,\n",
       " '##reen': 10902,\n",
       " 'push': 11604,\n",
       " 'into': 12609,\n",
       " 'va': 12949,\n",
       " '涉': 3868,\n",
       " 'ecfa': 12496,\n",
       " '##滙': 17059,\n",
       " '##瘦': 17664,\n",
       " '豁': 6485,\n",
       " '63': 8381,\n",
       " '##鈕': 20103,\n",
       " '##鑄': 20196,\n",
       " '##钞': 20220,\n",
       " '##ga': 8676,\n",
       " '##itor': 12449,\n",
       " '340': 10049,\n",
       " 'mtv': 11529,\n",
       " '##據': 16144,\n",
       " '##尺': 15280,\n",
       " '##抿': 15911,\n",
       " '[PAD]': 0,\n",
       " '##◢': 13617,\n",
       " '★': 480,\n",
       " '伸': 847,\n",
       " '睛': 4714,\n",
       " '##浇': 16899,\n",
       " '##代': 13864,\n",
       " '##熵': 17287,\n",
       " '潸': 4063,\n",
       " '281': 10971,\n",
       " '##吹': 14487,\n",
       " '##盃': 17713,\n",
       " '##憤': 15791,\n",
       " 'pony': 8365,\n",
       " '##น': 13444,\n",
       " '##スト': 10579,\n",
       " '艷': 5684,\n",
       " '##ⁱ': 13511,\n",
       " '蘭': 5984,\n",
       " '娓': 2022,\n",
       " '265': 8689,\n",
       " '##餉': 20677,\n",
       " '##冏': 14144,\n",
       " '芜': 5697,\n",
       " 'hk': 8512,\n",
       " 'overdope': 9964,\n",
       " '穫': 4953,\n",
       " '##倘': 14008,\n",
       " '##攸': 16177,\n",
       " '1080': 10680,\n",
       " '##弦': 15535,\n",
       " '涧': 3884,\n",
       " '##っと': 10475,\n",
       " '##且': 13741,\n",
       " '##鄭': 20029,\n",
       " '珞': 4402,\n",
       " '衍': 6122,\n",
       " '##watch': 9907,\n",
       " 'con': 11485,\n",
       " '蔭': 5923,\n",
       " '1926': 10126,\n",
       " '##紊': 18207,\n",
       " 'b12': 12699,\n",
       " '##悻': 15712,\n",
       " '旋': 3181,\n",
       " '班': 4408,\n",
       " '##例': 13948,\n",
       " '倍': 945,\n",
       " '##缮': 18427,\n",
       " '##雕': 20482,\n",
       " '##放': 16180,\n",
       " '##sy': 10178,\n",
       " '##觥': 19295,\n",
       " '踏': 6672,\n",
       " '〖': 528,\n",
       " '獠': 4358,\n",
       " '##se': 8417,\n",
       " 'ra': 12619,\n",
       " '博': 1300,\n",
       " '##仝': 13861,\n",
       " '傻': 1004,\n",
       " '啬': 1572,\n",
       " '睾': 4728,\n",
       " '耶': 5456,\n",
       " '衰': 6139,\n",
       " 'secret': 11634,\n",
       " '枕': 3359,\n",
       " '##厝': 14389,\n",
       " '##掘': 16020,\n",
       " '##炉': 17197,\n",
       " '##耿': 18518,\n",
       " '燈': 4236,\n",
       " '邢': 6928,\n",
       " '##据': 16002,\n",
       " '揆': 2986,\n",
       " '樓': 3559,\n",
       " '##pass': 13206,\n",
       " '尬': 2217,\n",
       " '♪': 493,\n",
       " '披': 2847,\n",
       " '##氧': 16766,\n",
       " '奚': 1949,\n",
       " 'english': 8899,\n",
       " '##朴': 16377,\n",
       " '鴿': 7863,\n",
       " '##慮': 15776,\n",
       " '４': 8032,\n",
       " '##癣': 17681,\n",
       " '##荚': 18835,\n",
       " 'น': 278,\n",
       " '懇': 2744,\n",
       " '碍': 4809,\n",
       " '##拷': 15950,\n",
       " '##托': 15862,\n",
       " '##藕': 19026,\n",
       " '##鐮': 20190,\n",
       " '氤': 3707,\n",
       " 'also': 12216,\n",
       " '##梨': 16516,\n",
       " '涝': 3876,\n",
       " '遼': 6910,\n",
       " '痍': 4572,\n",
       " '錚': 7090,\n",
       " '##胴': 18596,\n",
       " '祷': 4876,\n",
       " '瘸': 4613,\n",
       " '謐': 6337,\n",
       " '178': 9649,\n",
       " '##寮': 15242,\n",
       " '纠': 5272,\n",
       " '盔': 4666,\n",
       " '榴': 3534,\n",
       " '##ction': 9116,\n",
       " '憚': 2733,\n",
       " '狂': 4312,\n",
       " '##疽': 17621,\n",
       " '篩': 5072,\n",
       " '##ᄎ': 13465,\n",
       " '##谷': 19541,\n",
       " '1938': 9429,\n",
       " '##饗': 20700,\n",
       " '捩': 2944,\n",
       " '倭': 963,\n",
       " '磲': 4839,\n",
       " '陀': 7351,\n",
       " '霈': 7449,\n",
       " '骡': 7751,\n",
       " '##國': 14808,\n",
       " '##僚': 14069,\n",
       " '检': 3466,\n",
       " '判': 1161,\n",
       " '398': 11425,\n",
       " '歴': 3643,\n",
       " '床': 2414,\n",
       " '烤': 4171,\n",
       " '辱': 6802,\n",
       " '银': 7213,\n",
       " '##．': 21082,\n",
       " '耐': 5447,\n",
       " '甕': 4490,\n",
       " '##拆': 15915,\n",
       " '##駱': 20751,\n",
       " 'png': 10246,\n",
       " 'uc': 9955,\n",
       " '凱': 1134,\n",
       " 'program': 11738,\n",
       " '##必': 15610,\n",
       " '##潭': 17116,\n",
       " '##眞': 17752,\n",
       " '##魯': 20855,\n",
       " '##幼': 15462,\n",
       " '孢': 2107,\n",
       " '##▅': 13599,\n",
       " '咙': 1479,\n",
       " '##駁': 20741,\n",
       " '孑': 2095,\n",
       " '拎': 2865,\n",
       " '##颅': 20622,\n",
       " '##ny': 8680,\n",
       " '蛳': 6039,\n",
       " 'n1': 11356,\n",
       " 'о': 246,\n",
       " '賭': 6551,\n",
       " '##bt': 10161,\n",
       " '仅': 788,\n",
       " '讷': 6386,\n",
       " '199': 9092,\n",
       " '##篮': 18131,\n",
       " '##惬': 15733,\n",
       " '##讓': 19423,\n",
       " '厝': 1332,\n",
       " '##靂': 20525,\n",
       " '揶': 3002,\n",
       " '##ty': 8661,\n",
       " '址': 1770,\n",
       " '状': 4307,\n",
       " '鄔': 6967,\n",
       " '谚': 6464,\n",
       " '少': 2208,\n",
       " '乗': 732,\n",
       " 'crm': 9991,\n",
       " '##pmlast': 12138,\n",
       " '硅': 4795,\n",
       " 'pinterest': 8379,\n",
       " '62': 8356,\n",
       " '3300': 11543,\n",
       " 'karl': 13263,\n",
       " '##atic': 11080,\n",
       " 'coc': 11917,\n",
       " '##嬢': 15141,\n",
       " '陲': 7375,\n",
       " '盞': 4672,\n",
       " '##ncy': 10518,\n",
       " '##按': 15959,\n",
       " '##・': 13703,\n",
       " '##悠': 15698,\n",
       " '##曦': 16343,\n",
       " '##礡': 17902,\n",
       " '##签': 18098,\n",
       " '臘': 5626,\n",
       " 'v9': 11894,\n",
       " '##維': 18261,\n",
       " '胴': 5539,\n",
       " '##墩': 14932,\n",
       " '汹': 3747,\n",
       " '棄': 3468,\n",
       " '##娜': 15082,\n",
       " '##ღ': 13453,\n",
       " '##樑': 16615,\n",
       " '##譎': 19407,\n",
       " '##賬': 19607,\n",
       " 'en': 9447,\n",
       " '##堯': 14896,\n",
       " '##跖': 19709,\n",
       " '##颢': 20644,\n",
       " '##黎': 21001,\n",
       " '##胥': 18588,\n",
       " '迦': 6830,\n",
       " '稿': 4943,\n",
       " '氫': 3712,\n",
       " '啼': 1582,\n",
       " '##d': 8168,\n",
       " '？': 8043,\n",
       " '恃': 2603,\n",
       " 'soc': 11405,\n",
       " '##姨': 15064,\n",
       " '##泼': 16869,\n",
       " '##网': 18438,\n",
       " '鮪': 7802,\n",
       " 'ng': 8885,\n",
       " 'きな': 10423,\n",
       " '剷': 1202,\n",
       " '坂': 1771,\n",
       " '##簪': 18140,\n",
       " '擔': 3085,\n",
       " '妆': 1966,\n",
       " '##煎': 17260,\n",
       " '##虏': 19046,\n",
       " '蝙': 6073,\n",
       " '##鏤': 20187,\n",
       " '1994': 8447,\n",
       " 'read': 8649,\n",
       " '##閲': 20346,\n",
       " '##废': 15483,\n",
       " '88': 8302,\n",
       " '鐵': 7136,\n",
       " '景': 3250,\n",
       " '##nny': 13239,\n",
       " '##喬': 14662,\n",
       " '螂': 6082,\n",
       " 'ｗ': 8073,\n",
       " '##ᅨ': 13475,\n",
       " '##潺': 17121,\n",
       " '溜': 3977,\n",
       " '##侖': 13952,\n",
       " '##葳': 18934,\n",
       " '邯': 6935,\n",
       " '315': 9613,\n",
       " '盖': 4667,\n",
       " '##婢': 15100,\n",
       " '##層': 15308,\n",
       " '##颗': 20635,\n",
       " '馴': 7683,\n",
       " '##鰍': 20869,\n",
       " '##曇': 16336,\n",
       " '##骚': 20803,\n",
       " '挲': 2921,\n",
       " '～4': 12476,\n",
       " '##瘟': 17659,\n",
       " '##筛': 18090,\n",
       " '##槓': 16601,\n",
       " '##耗': 18507,\n",
       " '##髦': 20828,\n",
       " 'books': 11280,\n",
       " '术': 3318,\n",
       " '丙': 688,\n",
       " 'rmb': 10887,\n",
       " '##亏': 13812,\n",
       " '##訕': 19304,\n",
       " '1v': 12819,\n",
       " '##聊': 18521,\n",
       " '够': 1916,\n",
       " '##段': 16724,\n",
       " '的': 4638,\n",
       " '骥': 7753,\n",
       " '133': 9246,\n",
       " '禁': 4881,\n",
       " '##迁': 19867,\n",
       " 'puma': 11803,\n",
       " '##栀': 16456,\n",
       " '##琉': 17474,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转化为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转化为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding='max_length', max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding='max_length', max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx !=0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding='max_length', max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = ['弱小的我也有大梦想',\n",
    "        '有梦想谁都了不起',\n",
    "        '追逐梦想的心，比梦想本身，更可贵'\n",
    "]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 72.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 203 ms\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../../models/roberta-base-finetuned-dianping-chinese/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = '弱小的我也有大Dreaming!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../../models/roberta-base-finetuned-dianping-chinese/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained('../../models/roberta-base-finetuned-dianping-chinese/')\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='../../models/roberta-base-finetuned-dianping-chinese/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained('../../models/roberta-base-finetuned-dianping-chinese/', use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 688 ms\n",
      "Wall time: 671 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.67 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.03 s\n",
      "Wall time: 156 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.55 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\gitrepos\\transformers_in_action\\01-Getting-Started\\03-tokenizer\\tokenizer.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/gitrepos/transformers_in_action/01-Getting-Started/03-tokenizer/tokenizer.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m slow_tokenizer(sen, return_offsets_mapping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Envs\\py_cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2806\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2804\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2805\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2806\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2807\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2808\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Envs\\py_cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2912\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2893\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2894\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2909\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2910\u001b[0m     )\n\u001b[0;32m   2911\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2912\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2913\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2914\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2915\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2916\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2917\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2918\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2919\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2920\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2921\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2922\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2923\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2924\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2925\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2926\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2927\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2928\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2929\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2930\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2931\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Envs\\py_cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2985\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2975\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2976\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2977\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2978\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2982\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2983\u001b[0m )\n\u001b[1;32m-> 2985\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2986\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2987\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2988\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2989\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2990\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2991\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2992\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2993\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2994\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2995\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2996\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2997\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2998\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2999\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3000\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3001\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   3002\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   3003\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3004\u001b[0m )\n",
      "File \u001b[1;32mc:\\Envs\\py_cuda\\lib\\site-packages\\transformers\\tokenization_utils.py:700\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    695\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    696\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    697\u001b[0m             )\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m--> 700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    701\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    702\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    705\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    706\u001b[0m     )\n\u001b[0;32m    708\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    709\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特殊的Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载非huggingface官方模型，需要加 trust_remote_code=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='chatglm_tokenizer', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatglm_tokenizer\\\\tokenizer_config.json',\n",
       " 'chatglm_tokenizer\\\\special_tokens_map.json',\n",
       " 'chatglm_tokenizer\\\\vocab.txt',\n",
       " 'chatglm_tokenizer\\\\added_tokens.json',\n",
       " 'chatglm_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"chatglm_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chatglm_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 dreaming! [SEP]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
